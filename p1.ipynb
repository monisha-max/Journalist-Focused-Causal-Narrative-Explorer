{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/sridhrutitikkisetti/nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sridhrutitikkisetti/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 10788\n",
      "      File ID                                    Categories  \\\n",
      "0  test/14826                                         trade   \n",
      "1  test/14828                                         grain   \n",
      "2  test/14829                                crude, nat-gas   \n",
      "3  test/14832  corn, grain, rice, rubber, sugar, tin, trade   \n",
      "4  test/14833                             palm-oil, veg-oil   \n",
      "\n",
      "                                             Content  \n",
      "0  ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...  \n",
      "1  CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...  \n",
      "2  JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...  \n",
      "3  THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...  \n",
      "4  INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import pandas as pd\n",
    "\n",
    "# Download necessary datasets\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def reuters_to_dataframe():\n",
    "    data = []\n",
    "    for file_id in reuters.fileids():  # Loop through all articles\n",
    "        categories = ', '.join(reuters.categories(file_id))  # Join categories with commas\n",
    "        content = reuters.raw(file_id).strip()  # Strip unnecessary whitespace\n",
    "        data.append({\"File ID\": file_id, \"Categories\": categories, \"Content\": content})\n",
    "    \n",
    "    # Convert the data into a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Generate the table for the entire dataset\n",
    "df_reuters = reuters_to_dataframe()\n",
    "\n",
    "# Print the dataset summary and head\n",
    "print(f\"Total rows: {len(df_reuters)}\")\n",
    "print(df_reuters.head())  # Display the first few rows\n",
    "\n",
    "# Optional: Save to a CSV file\n",
    "df_reuters.to_csv(\"reuters_dataset_full.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4790345e5d6641238a912a46888d6dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 53792 sentences.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import BertTokenizer, BertForTokenClassification, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Tokenizer setup\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Prepare the Reuters dataset for training\n",
    "def preprocess_reuters_data(df):\n",
    "    sentences, labels = [], []\n",
    "    causal_indicators = [\"caused by\", \"led to\", \"due to\", \"triggered\", \"resulted in\", \"owing to\"]\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        content = row[\"Content\"]\n",
    "        category = row[\"Categories\"]\n",
    "\n",
    "        # Tokenize content into sentences\n",
    "        for sentence in sent_tokenize(content):\n",
    "            sentences.append(sentence)\n",
    "            # Simulated label: Check for causal indicators\n",
    "            label = int(any(indicator in sentence.lower() for indicator in causal_indicators))\n",
    "            labels.append(label)\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "sentences, labels = preprocess_reuters_data(df_reuters)\n",
    "print(f\"Preprocessed {len(sentences)} sentences.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReutersDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_len):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the sentence\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = ReutersDataset(sentences, labels, tokenizer, max_len=128)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b84eaa96d9f4006a1f286c8ede3c73d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.03399642606510377\n",
      "Epoch 2 - Loss: 0.01021203353680375\n",
      "Epoch 3 - Loss: 0.00817001307374709\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Load the model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} - Loss: {total_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to bert_event_trigger_model\n"
     ]
    }
   ],
   "source": [
    "# Directory to save the model\n",
    "save_directory = \"bert_event_trigger_model\"\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to load the model again\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(save_directory)\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully\")\n",
    "#to load the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to use the trained model\n",
    "# Example usage\n",
    "input_sentence = \"The market crash was caused by unexpected inflation.\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = loaded_tokenizer(\n",
    "    input_sentence, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_model(**inputs)\n",
    "\n",
    "# Get predicted label\n",
    "predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "label_map = {0: \"Non-Causal\", 1: \"Causal\"}\n",
    "print(f\"Predicted label: {label_map[predicted_label]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load BERT for token classification\n",
    "model_args = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)  # Adjust labels for arguments\n",
    "\n",
    "# Create token-level labels (0: O, 1: Trigger, 2: Argument)\n",
    "def generate_token_labels(sentence, causal_indicators):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    labels = [0] * len(tokens)\n",
    "\n",
    "    for indicator in causal_indicators:\n",
    "        if indicator in sentence.lower():\n",
    "            indicator_tokens = tokenizer.tokenize(indicator)\n",
    "            start_idx = sentence.lower().find(indicator)\n",
    "\n",
    "            for i, token in enumerate(tokens):\n",
    "                token_start = sentence.find(token)\n",
    "                if start_idx <= token_start < start_idx + len(indicator):\n",
    "                    labels[i] = 1  # Mark as Trigger\n",
    "                else:\n",
    "                    labels[i] = 2  # Mark as Argument\n",
    "    return tokens, labels\n",
    "\n",
    "# Generate token-level labels\n",
    "token_sentences, token_labels = [], []\n",
    "for sentence in sentences:\n",
    "    tokens, labels = generate_token_labels(sentence, [\"caused by\", \"led to\"])\n",
    "    token_sentences.append(tokens)\n",
    "    token_labels.append(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Causal       1.00      1.00      1.00     52682\n",
      "      Causal       0.96      0.99      0.97      1110\n",
      "\n",
      "    accuracy                           1.00     53792\n",
      "   macro avg       0.98      0.99      0.99     53792\n",
      "weighted avg       1.00      1.00      1.00     53792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Inference and evaluation\n",
    "model.eval()\n",
    "true_labels, pred_labels = [], []\n",
    "\n",
    "for batch in dataloader:\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits.argmax(dim=1).cpu().numpy()\n",
    "    true_labels.extend(batch[\"label\"].cpu().numpy())\n",
    "    pred_labels.extend(logits)\n",
    "\n",
    "print(classification_report(true_labels, pred_labels, target_names=[\"Non-Causal\", \"Causal\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Causal       1.00      1.00      1.00     52682\n",
      "      Causal       0.96      0.99      0.97      1110\n",
      "\n",
      "    accuracy                           1.00     53792\n",
      "   macro avg       0.98      0.99      0.99     53792\n",
      "weighted avg       1.00      1.00      1.00     53792\n",
      "\n"
     ]
    }
   ],
   "source": [
    
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
